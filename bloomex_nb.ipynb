{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eba4f35-ed22-4de8-8124-49f100a4a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomForTokenClassification\n",
    "from transformers import BloomForTokenClassification\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "#! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1ea361-84b4-4f6d-98c6-ec926b104240",
   "metadata": {},
   "source": [
    "# Code Summarization Evaluation Dataset Selection\n",
    "\n",
    "## [CodeSearchNet](https://github.com/github/CodeSearchNet#data-details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7eebc-6f48-4f61-9f29-0516188af184",
   "metadata": {},
   "source": [
    "Quality is catastrophically poor. \"docstrings\" are literally the docstrings from the associated functions written by devs. Quality varies massively, as is to be expected from GitHub. Many docstrings don't actually describe what the code does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06485972-4f1f-4b2e-8825-b7f1d2578973",
   "metadata": {},
   "source": [
    "## [code-docstring-corpus](https://github.com/EdinburghNLP/code-docstring-corpus)\n",
    "\n",
    "Without even looking at the data, this will likely suffer from the same problem. Code is scraped from GitHub and docstrings are extracted. Pass.\n",
    "\n",
    "## Initial Hypothesis\n",
    "\n",
    "It appears that the type and quality of \"code summarization' that would be useful to auditors (likely higher-level, more abstract but correct and informative) is largely absent from popular code summarization benchmark datasets.\n",
    "\n",
    "# Summary Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5533829b-7df9-49bd-9eff-04ff5987da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034b0c22-b11b-471c-ac42-2b60722453f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68946874, 0.3345313 , 0.39983743, 0.8875231 ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([['alpha', 'ALPHA-t'],\n",
    "        ['Beta', 'epsilon'],\n",
    "        ['That There Man', 'Who There man'],\n",
    "        ['Lets have cheese', 'Can we have cheese?']])\n",
    "\n",
    "def evaluate(y):\n",
    "    ''' Calculates the consine similarity of two sets of code summaries using MPNET-base encodings.\n",
    "    \n",
    "        Args:\n",
    "          y: a NumPy array (2, n) of predicted code summaries and the corresponding ground truth summary.\n",
    "        \n",
    "        Output:\n",
    "          A NumPy array (1, n) of sentence similarity scores from -1 (opposite) to 1 (proportional).\n",
    "    '''\n",
    "    \n",
    "    similarities = []\n",
    "    pred_embed = sentence_model.encode(y[:,0])\n",
    "    truth_embed = sentence_model.encode(y[:,1])\n",
    "    \n",
    "    for i in range(len(pred_embed)):\n",
    "        similarities.append(cosine_similarity([pred_embed[i]], [truth_embed[i]])[0][0])\n",
    "    \n",
    "    return np.array(similarities).reshape((1,len(similarities)))\n",
    "\n",
    "evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e306dd3-50db-43f1-b7ff-67c84d318535",
   "metadata": {},
   "source": [
    "# Bloom for Causal Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163302d1-6ebb-414c-b2a3-56bca74281ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only to re-download models if existing files are deleted\n",
    "# Use only to store model if the existing files are delected\n",
    "\n",
    "#tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b3\")\n",
    "#model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-1b3\")\n",
    "\n",
    "# Model Sizes (serialized byte stream)\n",
    "# 1b3: 6.4G \n",
    "# 2b5: #.#G (runs out of memory loading CausalLM)\n",
    "\n",
    "#pickle.dump(model, open('CausalLM_bloom-1b3.pkl', 'wb'))\n",
    "#pickle.dump(tokenizer, open('Tokenizer_bloom-1b3.pkl', 'wb'))\n",
    "#del model\n",
    "#del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf24551f-e164-400e-8daf-6fab1d0934f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pickle.load(open('Tokenizer_bloom-1b3.pkl', 'rb'))\n",
    "model = pickle.load(open('CausalLM_bloom-1b3.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7673f7-0a71-4f0f-b5f4-896250479dc2",
   "metadata": {},
   "source": [
    "| x | y | generated_text | novel_portion |\n",
    "|---|---|----------------|---------------|\n",
    "|   |   |                |               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1813d285-d912-46e1-8f3c-42fbda5fbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outline for functions\n",
    "\n",
    "def draw_sample(corpus, k, n, code_len_max, docstring_len_max):\n",
    "    '''\n",
    "        corpus: filename of data corpus\n",
    "        k: int. sample size\n",
    "        n: tuple. min and max number of observations that meet other args' criteria to keep\n",
    "        code_len_max: filter sample for observations whose code len is below code_len_max\n",
    "        docstring_len_max: filter sample for observations whose docstring len is below docstring_len_max\n",
    "    '''\n",
    "    \n",
    "    sample_tmp = []\n",
    "\n",
    "    with open(corpus, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    sample_indices = random.choices(range(0, len(json_list)), k=k)\n",
    "\n",
    "    for i in sample_indices:\n",
    "        docstring = json.loads(json_list[i])['docstring']\n",
    "        code = json.loads(json_list[i])['code']\n",
    "        if len(code) < code_len_max and len(docstring) < docstring_len_max:\n",
    "\n",
    "            # Remove docstring from code\n",
    "            if code.find('\"\"\"') != -1:\n",
    "                start = code.index('\"\"\"')\n",
    "                end = code[start + 3:].index('\"\"\"') + start + 6\n",
    "                clean_code = code[:start] + code[end:]\n",
    "            elif code.find(\"'''\") != -1:\n",
    "                start = code.index(\"'''\")\n",
    "                end = code[start + 3:].index(\"'''\") + start + 6\n",
    "                clean_code = code[:start] + code[end:]\n",
    "            else:\n",
    "                clean_code = code\n",
    "\n",
    "            sample_tmp.append([clean_code, docstring])\n",
    "\n",
    "    json_file.close()\n",
    "    print(len(sample_tmp))\n",
    "    \n",
    "    if n[0] <= len(sample_tmp) <= n[1]:\n",
    "        return sample_tmp\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def promptize(examples):\n",
    "    '''\n",
    "        Turns an array of [[code, docstring]] examples into a single LLM prompt, dropping the last\n",
    "        row's docstring as the target to be predicted.\n",
    "    '''\n",
    "    res = []\n",
    "    for i, ex in enumerate(examples):\n",
    "        if i < len(examples) - 1:\n",
    "            res.append(ex[0] + '\\n\\n##' + ex[1])\n",
    "        else:\n",
    "            res.append(ex[0] + '\\n##')\n",
    "    return np.array(res).reshape((1, len(examples)))\n",
    "\n",
    "def generate(seed, output_max_tokens):\n",
    "    '''\n",
    "        Takes a prompt, tokenizes it, calls Bloom and returns a predicted (generated) code summary.\n",
    "    '''\n",
    "    prompt = \"\"\n",
    "\n",
    "    for p in promptize(seed)[0]:\n",
    "        prompt += p + '\\n\\n'\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    result_length = len(inputs.tokens()) + output_max_tokens\n",
    "    \n",
    "    g = tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                         max_length=result_length,\n",
    "                         no_repeat_ngram_size=4,\n",
    "                         skip_special_tokens=True,\n",
    "                         early_stopping=True\n",
    "                         )[0])\n",
    "    return g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d653d912-7049-4e21-9817-4a790fdd91a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "y : Get Existing Message\n",
      "\n",
      "        http://dev.wheniwork.com/#get-existing-message\n",
      "ŷ : def get_messages(self, start, end):\n",
      "Cosine similarity: 0.55403036\n"
     ]
    }
   ],
   "source": [
    "sample_tmp = draw_sample(corpus='./python_valid_0.jsonl', \n",
    "                         k=20, \n",
    "                         n=(2, 6), \n",
    "                         code_len_max=300, \n",
    "                         docstring_len_max=100)\n",
    "\n",
    "if sample_tmp is None:\n",
    "    print('Sampling criteria not met.')\n",
    "else:\n",
    "    # Generated a prediction\n",
    "    y_hat = generate(sample_tmp, 25)\n",
    "    \n",
    "    # Find best match across all predicted lines\n",
    "    sims = []\n",
    "    threshold = 0.35\n",
    "    \n",
    "    prompt = \"\"\n",
    "\n",
    "    # TEMP NEED TO REMOVE\n",
    "    for p in promptize(sample_tmp)[0]:\n",
    "        prompt += p + '\\n\\n'\n",
    "\n",
    "    for line in y_hat[len(prompt):].split('\\n'):\n",
    "        s = evaluate(np.array([[line, sample_tmp[len(sample_tmp) - 1][1]]]))\n",
    "        sims.append(s)\n",
    "\n",
    "    max_similarity = np.amax(np.array(sims))\n",
    "    max_similarity_index = sims.index(max_similarity)\n",
    "\n",
    "    #if max_similarity >= threshold:\n",
    "    print('y :',sample_tmp[len(sample_tmp) - 1][1])\n",
    "    print('y\\u0302 :', y_hat[len(prompt):].split('\\n')[max_similarity_index].strip())\n",
    "    print('Cosine similarity:', max_similarity)\n",
    "    #else:\n",
    "    #    print('No match above threshold.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9ebb8-7026-40de-9c53-446a09166094",
   "metadata": {},
   "source": [
    "**Debugging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea3fd1e-2789-4ef6-80d4-427a954ad763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def decode(cls, decrypted):\n",
      "        \n",
      "        pad = ord(decrypted[-1])\n",
      "        if pad < 1 or pad > 32:\n",
      "            pad = 0\n",
      "        return decrypted[:-pad]\n",
      "\n",
      "##删除解密后明文的补位字符\n",
      "        @param decrypted: 解密后的明文\n",
      "        @return: 删除补位字符后的明文\n",
      "\n",
      "def signal_receive(self, fd):\n",
      "        \n",
      "        connections = self.connections\n",
      "        if connections(fd) and self.twait[connections(fd)]:\n",
      "            procid = random.sample(self.twait[connections(fd)], 1)[0]\n",
      "            self.awake(procid)\n",
      "\n",
      "##Awake one process waiting to receive data on fd\n",
      "\n",
      "def empty(self):\n",
      "        \n",
      "        self.mutex.acquire()\n",
      "        n = not self._qsize()\n",
      "        self.mutex.release()\n",
      "        return n\n",
      "##\n",
      "\n",
      "class Process(threading.Thread):\n",
      "    \n",
      "    def __init__(self, pid, name, args, kwargs):\n",
      "        \n",
      "       \n"
     ]
    }
   ],
   "source": [
    "# Prediction (prompt + generated text)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb19c9bc-c7e1-4c8e-bf3c-911a663f57b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def decode(cls, decrypted):\n",
      "        \n",
      "        pad = ord(decrypted[-1])\n",
      "        if pad < 1 or pad > 32:\n",
      "            pad = 0\n",
      "        return decrypted[:-pad]\n",
      "\n",
      "##删除解密后明文的补位字符\n",
      "        @param decrypted: 解密后的明文\n",
      "        @return: 删除补位字符后的明文\n",
      "\n",
      "def signal_receive(self, fd):\n",
      "        \n",
      "        connections = self.connections\n",
      "        if connections(fd) and self.twait[connections(fd)]:\n",
      "            procid = random.sample(self.twait[connections(fd)], 1)[0]\n",
      "            self.awake(procid)\n",
      "\n",
      "##Awake one process waiting to receive data on fd\n",
      "\n",
      "def empty(self):\n",
      "        \n",
      "        self.mutex.acquire()\n",
      "        n = not self._qsize()\n",
      "        self.mutex.release()\n",
      "        return n\n",
      "##\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4f80df6-acb4-4d1a-9699-de92b2930c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return True if the queue is empty, False otherwise (not reliable!).\n"
     ]
    }
   ],
   "source": [
    "# Ground truth comment\n",
    "print(sample_tmp[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a36baa-db6a-45a8-a449-3a439fe645a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DELETE\n",
    "#prompt = \"\"\n",
    "\n",
    "#for p in promptize(sample_tmp)[0]:\n",
    "#    prompt += p + '\\n\\n'\n",
    "\n",
    "#print('GROUND TRUTH:', sample_tmp[len(sample_tmp) - 1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60eb2324-38f7-4a8c-afca-683c8fce0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DELETE\n",
    "#inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "#result_length = len(inputs.tokens()) + 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db68bc2-f556-4353-a40a-28db2f71cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DELETE\n",
    "# Greedy Search\n",
    "#y_hat = tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "#                       max_length=result_length,\n",
    "#                       no_repeat_ngram_size=4,\n",
    "#                       skip_special_tokens=True,\n",
    "#                       early_stopping=True\n",
    "#                      )[0])\n",
    "#print(y_hat[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201f288-a524-4cb9-9155-1b46893a0f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just take first line from prediction\n",
    "# Deprecated in favor of search method above\n",
    "\n",
    "#novel_bit = y_hat[len(prompt):]\n",
    "#novel_bit = novel_bit[:novel_bit.find('\\n')].replace('##','')\n",
    "#print('y :',sample_tmp[len(sample_tmp) - 1][1])\n",
    "#print('y\\u0302 :',novel_bit)\n",
    "\n",
    "#print('Cosine Similarity:', *evaluate(np.array([[novel_bit, sample_tmp[len(sample_tmp) - 1][1]]]))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7a0cc-2682-423a-97de-579944703bee",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84e419a6-92ff-44cb-bfee-22710fc1facc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Draw large sample from dataset\n",
    "\n",
    "sample_tmp = draw_sample(corpus='./python_valid_0.jsonl', \n",
    "                         k=200, \n",
    "                         n=(12, 40), \n",
    "                         code_len_max=300, \n",
    "                         docstring_len_max=150)\n",
    "\n",
    "if not sample_tmp is None:\n",
    "    print(len(sample_tmp))\n",
    "\n",
    "# Split into n sized groups (n=number of predictions desired)\n",
    "\n",
    "\n",
    "# For each group:\n",
    "  # Process into prompt\n",
    "  # Run inference\n",
    "  # Calculate similarity between y and y_hat\n",
    "\n",
    "# Store large sample: (x, y, y_hat, similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ff1752e-484f-4dc8-9c44-f118b1a6f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_shots: The number of examples to provide in a single prompt.\n",
    "# Takes a sample and divides it into sets of examples of size num_shots. Drops examples from sample until it is divisible by num_shots.\n",
    "\n",
    "num_shots = 4\n",
    "\n",
    "num_obs = len(sample_tmp)\n",
    "\n",
    "while num_obs % num_shots != 0:\n",
    "    sample_tmp = sample_tmp[:num_obs-1]\n",
    "    num_obs = len(sample_tmp)\n",
    "    \n",
    "prompts = []\n",
    "\n",
    "for i in range(0,len(sample_tmp), num_shots):\n",
    "    prompts.append(sample_tmp[i:i+num_shots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6443d9c5-c795-4634-be42-38c9a1370855",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    predictions.append(generate(prompt, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29562ec7-fc25-490f-bda9-7ff13049d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def Element(self):\n",
      "        \n",
      "        if not self._element:\n",
      "            self.Refind(maxSearchSeconds=TIME_OUT_SECOND, searchIntervalSeconds=self.searchWaitTime)\n",
      "        return self._element\n",
      "\n",
      "##Property Element.\n",
      "        Return `ctypes.POINTER(IUIAutomationElement)`.\n",
      "\n",
      "def rewindbody(self):\n",
      "        \n",
      "        if not self.seekable:\n",
      "            raise IOError, \"unseekable file\"\n",
      "        self.fp.seek(self.startofbody)\n",
      "\n",
      "##Rewind the file to the start of the body (if seekable).\n",
      "\n",
      "def make_tarfile(output_filename, source_dir):\n",
      "  \n",
      "  with tarfile.open(output_filename, \"w:gz\") as tar:\n",
      "    tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
      "\n",
      "##Tar a directory\n",
      "\n",
      "def _cell(x):\n",
      "    \n",
      "    x_no_none = [i if i is not None else \"\" for i in x]\n",
      "    return array(x_no_none, dtype=np_object)\n",
      "##\n",
      "\n",
      "\n",
      "def isnumber(self, string, *args):\n",
      "        \n",
      "        try:\n",
      "            n, u = utility.analyze_number(string)\n",
      "        except SyntaxError:\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "##Is number\n",
      "        args:\n",
      "            string (str): match\n",
      "        returns:\n",
      "            bool\n",
      "\n",
      "def initialize(self):\n",
      "        \n",
      "        return self.DXclasses[self.type](self.id,**self.args)\n",
      "\n",
      "##Initialize the corresponding DXclass from the data.\n",
      "\n",
      "        class = DXInitObject.initialize()\n",
      "\n",
      "def zplane(self,auto_scale=True,size=2,detect_mult=True,tol=0.001):\n",
      "        \n",
      "        iir_d.sos_zplane(self.sos,auto_scale,size,tol)\n",
      "\n",
      "##Plot the poles and zeros of the FIR filter in the z-plane\n",
      "\n",
      "def from_devanagari(self, data):\n",
      "        \n",
      "        from indic_transliteration import sanscript\n",
      "        return sanscript.transliterate(data=data, _from=sanscript.DEVANAGARI, _to=self.name)\n",
      "##\n",
      "\n",
      "\n",
      "def interest_accrued(self, valuation_date):\n",
      "        \n",
      "        return sum([l.interest_accrued(valuation_date) for l in self.legs if hasattr(l, 'interest_accrued')])\n",
      "\n",
      "##interest_accrued\n",
      "        :param valuation_date:\n",
      "        :type valuation_date:\n",
      "        :return:\n",
      "        :rtype:\n",
      "\n",
      "def random_product(iter1, iter2):\n",
      "    \n",
      "    pool1 = tuple(iter1)\n",
      "    pool2 = tuple(iter2)\n",
      "    ind1 = random.sample(pool1, 2)\n",
      "    ind2 = random.sample(pool2, 2)\n",
      "    return tuple(ind1+ind2)\n",
      "\n",
      "##random sampler for equal_splits func\n",
      "\n",
      "def set_aad_cache(token, cache):\n",
      "    \n",
      "    set_config_value('aad_token', jsonpickle.encode(token))\n",
      "    set_config_value('aad_cache', jsonpickle.encode(cache))\n",
      "\n",
      "##Set AAD token cache.\n",
      "\n",
      "def setup(self, pin, mode):\n",
      "        \n",
      "        self._setup_pin(pin, mode)\n",
      "        self.mpsse_write_gpio()\n",
      "##\n",
      "\n",
      "\n",
      "def conference_hangup(self, call_params):\n",
      "        \n",
      "        path = '/' + self.api_version + '/ConferenceHangup/'\n",
      "        method = 'POST'\n",
      "        return self.request(path, method, call_params)\n",
      "\n",
      "##REST Conference Hangup helper\n",
      "\n",
      "def bookmark(ctx, username):  # pylint:disable=redefined-outer-name\n",
      "    \n",
      "    ctx.obj = ctx.obj or {}\n",
      "    ctx.obj['username'] = username\n",
      "\n",
      "##Commands for bookmarks.\n",
      "\n",
      "def banner(msg):\n",
      "    \n",
      "    if ECHO:\n",
      "        _flush()\n",
      "        sys.stderr.write(\"\\033[1;7;32;40m{}\\033[0m\\n\".format(msg))\n",
      "        sys.stderr.flush()\n",
      "\n",
      "##Emit a banner just like Invoke's `run(…, echo=True)`.\n",
      "\n",
      "def config_dir(self):\n",
      "        \n",
      "        home = expanduser('~')\n",
      "        config_dir = os.path.join(home, '.jackal')\n",
      "        return config_dir\n",
      "##\n",
      "\n",
      "\n",
      "def p_pkg_home_value_1(self, p):\n",
      "        \n",
      "        if six.PY2:\n",
      "            p[0] = p[1].decode(encoding='utf-8')\n",
      "        else:\n",
      "            p[0] = p[1]\n",
      "\n",
      "##pkg_home_value : LINE\n",
      "\n",
      "def time(self, t):\n",
      "        \n",
      "        _time = arrow.get(t).format('YYYY-MM-DDTHH:mm:ss')\n",
      "        self._time = datetime.datetime.strptime(_time, '%Y-%m-%dT%H:%M:%S')\n",
      "\n",
      "##Convert any timestamp into a datetime and save as _time\n",
      "\n",
      "def process(self, tensor):\n",
      "        \n",
      "        for processor in self.preprocessors:\n",
      "            tensor = processor.process(tensor=tensor)\n",
      "        return tensor\n",
      "\n",
      "##Process state.\n",
      "\n",
      "        Args:\n",
      "            tensor: tensor to process\n",
      "\n",
      "        Returns: processed state\n",
      "\n",
      "def construct(cls, project, **desc):\n",
      "        \n",
      "        return cls(project.drivers, maker=project.maker, **desc)\n",
      "##\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3296/3269692060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpromptize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3296/2404970862.py\u001b[0m in \u001b[0;36mpromptize\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n\\n##'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n##'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "for i in prompts:\n",
    "    x = ''\n",
    "    for p in promptize(i)[0]:\n",
    "            x += p + '\\n\\n'\n",
    "\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "33222784-187d-4e03-b77f-c33f26557cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['def Element(self):\\n        \\n        if not self._element:\\n            self.Refind(maxSearchSeconds=TIME_OUT_SECOND, searchIntervalSeconds=self.searchWaitTime)\\n        return self._element',\n",
       "        'Property Element.\\n        Return `ctypes.POINTER(IUIAutomationElement)`.'],\n",
       "       ['def rewindbody(self):\\n        \\n        if not self.seekable:\\n            raise IOError, \"unseekable file\"\\n        self.fp.seek(self.startofbody)',\n",
       "        'Rewind the file to the start of the body (if seekable).'],\n",
       "       ['def make_tarfile(output_filename, source_dir):\\n  \\n  with tarfile.open(output_filename, \"w:gz\") as tar:\\n    tar.add(source_dir, arcname=os.path.basename(source_dir))',\n",
       "        'Tar a directory'],\n",
       "       ['def _cell(x):\\n    \\n    x_no_none = [i if i is not None else \"\" for i in x]\\n    return array(x_no_none, dtype=np_object)',\n",
       "        'translate an array x into a MATLAB cell array']], dtype='<U187')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8101d6e-7d1f-49fd-907f-90f69770d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    out.append([prompts[i][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1277f-0d99-465b-80e6-e63b860338a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "sims = []\n",
    "# TEMP NEED TO REMOVE\n",
    "for p in promptize(sample_tmp)[0]:\n",
    "    prompt += p + '\\n\\n'\n",
    "\n",
    "for line in y_hat[len(prompt):].split('\\n'):\n",
    "    s = evaluate(np.array([[line, sample_tmp[len(sample_tmp) - 1][1]]]))\n",
    "    sims.append(s)\n",
    "\n",
    "max_similarity = np.amax(np.array(sims))\n",
    "max_similarity_index = sims.index(max_similarity)\n",
    "\n",
    "#if max_similarity >= threshold:\n",
    "print('y :',sample_tmp[len(sample_tmp) - 1][1])\n",
    "print('y\\u0302 :', y_hat[len(prompt):].split('\\n')[max_similarity_index].strip())\n",
    "    print('Cosine similarity:', max_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbec9d50-b5ff-4cec-b202-2961a61d0231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def GetPreviousSiblingControl(self) -> 'Control':\n",
      "        \n",
      "        ele = _AutomationClient.instance().ViewWalker.GetPreviousSiblingElement(self.Element)\n",
      "        return Control.CreateControlFromElement(ele)\n",
      "\n",
      "##Return `Control` subclass or None.\n",
      "\n",
      "def transmit_content_metadata(self, user):\n",
      "        \n",
      "        exporter = self.get_content_metadata_exporter(user)\n",
      "        transmitter = self.get_content_metadata_transmitter()\n",
      "        transmitter.transmit(exporter.export())\n",
      "\n",
      "##Transmit content metadata to integrated channel.\n",
      "\n",
      "def _send(self, stanza):\n",
      "        \n",
      "        self.fix_out_stanza(stanza)\n",
      "        element = stanza.as_xml()\n",
      "        self._write_element(element)\n",
      "\n",
      "##Same as `send` but assume `lock` is acquired.\n",
      "\n",
      "def _f_gene(sid, prefix=\"G_\"):\n",
      "    \n",
      "    sid = sid.replace(SBML_DOT, \".\")\n",
      "    return _clip(sid, prefix)\n",
      "##\n",
      "\n",
      "def _clip(s, prefix):\n",
      "    \n",
      "    s = s.replace(\" \", \"_\")\n",
      "    s = s[:\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4aee453-926e-43c2-9abb-26691f5a9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def GetPreviousSiblingControl(self) -> 'Control':\n",
      "        \n",
      "        ele = _AutomationClient.instance().ViewWalker.GetPreviousSiblingElement(self.Element)\n",
      "        return Control.CreateControlFromElement(ele) \n",
      "\n",
      " Return `Control` subclass or None. \n",
      "\n",
      "\n",
      "def transmit_content_metadata(self, user):\n",
      "        \n",
      "        exporter = self.get_content_metadata_exporter(user)\n",
      "        transmitter = self.get_content_metadata_transmitter()\n",
      "        transmitter.transmit(exporter.export()) \n",
      "\n",
      " Transmit content metadata to integrated channel. \n",
      "\n",
      "\n",
      "def _send(self, stanza):\n",
      "        \n",
      "        self.fix_out_stanza(stanza)\n",
      "        element = stanza.as_xml()\n",
      "        self._write_element(element) \n",
      "\n",
      " Same as `send` but assume `lock` is acquired. \n",
      "\n",
      "\n",
      "def _f_gene(sid, prefix=\"G_\"):\n",
      "    \n",
      "    sid = sid.replace(SBML_DOT, \".\")\n",
      "    return _clip(sid, prefix) \n",
      "\n",
      " Clips gene prefix from id. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in sample_tmp[0:4]:\n",
    "    print(p[0], '\\n\\n', p[1], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201fe9a9-c1f0-495d-97e4-57166877e97a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "venv",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
